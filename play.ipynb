{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from retnet.configuration_retnet import RetNetConfig\n",
    "from retnet.modeling_retnet import RetNetModel, RetNetModelWithLMHead\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "config = RetNetConfig(num_layers=8,\n",
    "                      hidden_size=512,\n",
    "                      num_heads=4,\n",
    "                      qk_dim=512,\n",
    "                      v_dim=1024,\n",
    "                      ffn_proj_size=1024,\n",
    "                      use_default_gamma=False)\n",
    "\n",
    "model = RetNetModel(config)\n",
    "model.eval()\n",
    "\n",
    "device = 'cuda'  # cuda, cpu, mps for m1 mac\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "layer: 1\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 2\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 3\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 4\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 5\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 6\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 7\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 8\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([[1,2,3,4,1,2,3,4]]).to(device)\n",
    "\n",
    "parallel_outputs = model(input_ids, forward_impl='parallel', use_cache=True)\n",
    "parallel_state = parallel_outputs.last_hidden_state\n",
    "parallel_cache = parallel_outputs.past_key_values\n",
    "\n",
    "past_kv = None\n",
    "rnn_state = []\n",
    "for i in range(input_ids.shape[1]):\n",
    "    rnn_out = model(input_ids[:, i:i+1], forward_impl='recurrent', past_key_values=past_kv, use_cache=True, sequence_offset=i)\n",
    "    rnn_state.append(rnn_out.last_hidden_state)\n",
    "    past_kv = rnn_out.past_key_values\n",
    "rnn_state = torch.cat(rnn_state, dim=1)\n",
    "rnn_cache = rnn_out.past_key_values\n",
    "\n",
    "\n",
    "chunk_outputs = model(input_ids, forward_impl='chunkwise', use_cache=True, chunk_size=4)\n",
    "chunk_state = chunk_outputs.last_hidden_state\n",
    "chunk_cache = chunk_outputs.past_key_values\n",
    "\n",
    "print(torch.allclose(parallel_state, rnn_state, atol=1e-5))\n",
    "print(torch.allclose(parallel_state, chunk_state, atol=1e-5))\n",
    "print(torch.allclose(rnn_state, chunk_state, atol=1e-5))\n",
    "\n",
    "for i, (p, r, c) in enumerate(zip(parallel_cache, rnn_cache, chunk_cache)):\n",
    "    print(f\"layer: {i + 1}\")\n",
    "    print(torch.allclose(p, r, atol=1e-5))\n",
    "    print(torch.allclose(p, c, atol=1e-5))\n",
    "    print(torch.allclose(r, c, atol=1e-5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "layer: 1\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 2\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 3\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 4\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 5\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 6\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 7\n",
      "True\n",
      "True\n",
      "True\n",
      "layer: 8\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([[1,2,3,4,1,2,3,4]]).to(device)\n",
    "retention_mask = torch.LongTensor([[1,1,1,1,1,0,0,0]]).to(device)\n",
    "\n",
    "parallel_outputs = model(input_ids, retention_mask=retention_mask, forward_impl='parallel', use_cache=True)\n",
    "parallel_state = parallel_outputs.last_hidden_state\n",
    "parallel_cache = parallel_outputs.past_key_values\n",
    "\n",
    "past_kv = None\n",
    "rnn_state = []\n",
    "for i in range(input_ids.shape[1]):\n",
    "    rnn_out = model(input_ids[:, i:i+1], retention_mask=retention_mask[:, i:i+1], forward_impl='recurrent', past_key_values=past_kv, use_cache=True, sequence_offset=i)\n",
    "    rnn_state.append(rnn_out.last_hidden_state)\n",
    "    past_kv = rnn_out.past_key_values\n",
    "rnn_state = torch.cat(rnn_state, dim=1)\n",
    "rnn_cache = rnn_out.past_key_values\n",
    "\n",
    "\n",
    "chunk_outputs = model(input_ids, retention_mask=retention_mask, forward_impl='chunkwise', use_cache=True, chunk_size=4)\n",
    "chunk_state = chunk_outputs.last_hidden_state\n",
    "chunk_cache = chunk_outputs.past_key_values\n",
    "\n",
    "print(torch.allclose(parallel_state, rnn_state, atol=1e-5))\n",
    "print(torch.allclose(parallel_state, chunk_state, atol=1e-5))\n",
    "print(torch.allclose(rnn_state, chunk_state, atol=1e-5))\n",
    "\n",
    "for i, (p, r, c) in enumerate(zip(parallel_cache, rnn_cache, chunk_cache)):\n",
    "    print(f\"layer: {i + 1}\")\n",
    "    print(torch.allclose(p, r, atol=1e-5))\n",
    "    print(torch.allclose(p, c, atol=1e-5))\n",
    "    print(torch.allclose(r, c, atol=1e-5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[29772, 22884,   842, 49295, 42077,  1681, 43525, 38407, 50177, 24993,\n",
       "          27512, 35252, 28429,  6718, 36836, 24775, 42771,    46, 13646,  2228]],\n",
       "        device='cuda:0'),\n",
       " tensor([[29772, 22884,   842, 49295, 42077,  1681, 43525, 38407, 50177, 24993,\n",
       "          27512, 35252, 28429,  6718, 36836, 24775, 42771,    46, 13646,  2228]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = RetNetModelWithLMHead(config).to(device)\n",
    "model.eval()\n",
    "\n",
    "p_generated = model.generate(input_ids, parallel_compute_prompt=True, max_new_tokens=20, do_sample=False, early_stopping=False)\n",
    "r_generated = model.generate(input_ids, parallel_compute_prompt=False, max_new_tokens=20, do_sample=False, early_stopping=False)\n",
    "\n",
    "p_generated, r_generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
