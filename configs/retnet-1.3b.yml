vocab_size: 50257
decoder_embed_dim: 2048
decoder_layers: 24
decoder_retention_heads: 8
decoder_ffn_embed_dim: 4096
initializer_range: 0.02
layernorm_eps: 1.0e-5
activation_fn: "gelu"
dropout: 0.0
activation_dropout: 0.0
drop_path_rate: 0.0
value_factor: 2
decoder_normalize_before: True
layernorm_embedding: True
no_scale_embedding: False
recurrent_chunk_size: 512
use_lm_decay: False
deepnorm: False
subln: True
output_retentions: False
pad_token_id: 50256
eos_token_id: 50256
unk_token_id: 50256
