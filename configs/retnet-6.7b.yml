vocab_size: 50257
decoder_embed_dim: 4096
decoder_layers: 32
decoder_retention_heads: 16
decoder_ffn_embed_dim: 8192
initializer_range: 0.02
layernorm_eps: 1.0e-5
activation_fn: "gelu"
dropout: 0.0
activation_dropout: 0.0
drop_path_rate: 0.0
value_factor: 2
decoder_normalize_before: True
layernorm_embedding: True
no_scale_embedding: False
recurrent_chunk_size: 512
use_lm_decay: False
deepnorm: False
subln: True
output_retentions: False
pad_token_id: 50256
eos_token_id: 50256
unk_token_id: 50256
